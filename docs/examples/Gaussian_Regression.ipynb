{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw1MqoCitdXu"
      },
      "source": [
        "# Basic Walkthrough - Gaussian Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Xk9VghtdXv"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/StatMixedML/XGBoostLSS/blob/master/docs/examples/Gaussian_Regression.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA1tMFMwtdXw"
      },
      "source": [
        "In this example, we model and predict all parameters of a univariate Normal distribution. Recall that distributional regression models and predicts all parameters $\\theta_{ik}, k=1, \\ldots, K$ parameters of a distribution $\\mathcal{D}$ as a function of covariates:\n",
        "\n",
        "\\begin{equation}\n",
        "y_{i} \\stackrel{ind}{\\sim} \\mathcal{D}\n",
        "  \\begin{pmatrix}\n",
        "     h_{1}\\bigl(\\theta_{i1}(x_{i})\\bigr) = \\eta_{i1} \\\\\n",
        "    h_{2}\\bigl(\\theta_{i2}(x_{i})\\bigr) = \\eta_{i2}  \\\\\n",
        "\t\\vdots \\\\                        \n",
        "\th_{K}\\bigl(\\theta_{iK}(x_{i})\\bigr) = \\eta_{iK}\n",
        "\\end{pmatrix}\n",
        "\\quad  ,i=1, \\ldots, N.\n",
        "\\end{equation}\n",
        "\n",
        "where $h_{k}(\\cdot)$ transforms each distributional parameter to the corresponding parameter scale. For the univariate Normal case, we can specify the above as $y_{i} \\stackrel{ind}{\\sim} \\mathcal{N}\\bigl(\\mu_{i}(x_{i}), \\sigma_{i}(x_{i})\\bigr)$. Since $\\mu_{i}(\\cdot) \\in \\mathbb{R}$ and since the standard-deviation cannot be negative, $h_{k}(\\cdot)$ is applied to $\\sigma_{i}(\\cdot)$ only. Typical choices are the exponential or the softplus function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwSRKthQtdXw"
      },
      "source": [
        "# Imports\n",
        "\n",
        "First, we import the necessary functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:24:10.418630300Z",
          "start_time": "2023-05-18T06:24:10.403008900Z"
        },
        "id": "YC9O9N5ctdXw"
      },
      "outputs": [],
      "source": [
        "from xgboostlss.model import *\n",
        "\n",
        "from xgboostlss.distributions.Gaussian import *\n",
        "from xgboostlss.datasets.data_loader import load_simulated_gaussian_data\n",
        "from scipy.stats import norm\n",
        "\n",
        "import multiprocessing\n",
        "import plotnine\n",
        "from plotnine import *\n",
        "plotnine.options.figure_size = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V25hkCPHtdXx"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0R5DZZtdXx"
      },
      "source": [
        "The data is simulated as a Gaussian, where $x_{true}$ is the only true feature and all others are noise variables:\n",
        "- $\\mu(x_{true}) = 10$\n",
        "- $\\sigma(x_{true}) = 1 + 4 * \\bigr((0.3 < x_{true}) \\& (x_{true} < 0.5)\\bigl) + 2 * (x_{true} > 0.7)$\n",
        "\n",
        "We first load the simulated dataset, filter for the target and covariates and then create the `xgb.DMatrix`. XGBoostLSS is designed to closely resemble the usage of XGBoost, ensuring ease of adoption and full compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:12:13.097935100Z",
          "start_time": "2023-05-18T06:12:03.538184Z"
        },
        "id": "JBzSsx9ItdXy"
      },
      "outputs": [],
      "source": [
        "train, test = load_simulated_gaussian_data()\n",
        "n_cpu = multiprocessing.cpu_count()\n",
        "\n",
        "X_train, y_train = train.filter(regex=\"x\"), train[\"y\"].values\n",
        "X_test, y_test = test.filter(regex=\"x\"), test[\"y\"].values\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, nthread=n_cpu)\n",
        "dtest = xgb.DMatrix(X_test, nthread=n_cpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuGjrPdOtdXy"
      },
      "source": [
        "# Distribution Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Wl8ALftdXy"
      },
      "source": [
        "Next, we specify a Gaussian distribution. By modifying the speciÔ¨Åcation in the following, the user can specify alternative distributional assumptions. This includes the option to choose from a wide range of parametric univariate and multivariate distributions, as well as to model the data using Normalizing Flows. The user also has different function arguments for each distribution:\n",
        "\n",
        "- `stabilization`: specifies the stabilization method for the Gradient and Hessian. Options are `None`, `MAD` and `L2`.\n",
        "- `response_fn`: specifies $h_{k}(\\cdot)$ and transforms the distributional parameter to the correct support. Here, we specify an exponential for $\\sigma_{i}(\\cdot)$ only.\n",
        "- `loss_fn`: specifies the loss function used for training.  Options are `nll` (negative log-likelihood) or `crps` (continuous ranked probability score).\n",
        "\n",
        "For additional details, see `?Gaussian`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:12:13.097935100Z",
          "start_time": "2023-05-18T06:12:04.423429800Z"
        },
        "id": "9UQowDv9tdXy"
      },
      "outputs": [],
      "source": [
        "xgblss = XGBoostLSS(\n",
        "    Gaussian(stabilization=\"None\",\n",
        "             response_fn=\"exp\",\n",
        "             loss_fn=\"nll\"\n",
        "            )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-OjFLtYtdXy"
      },
      "source": [
        "# Hyper-Parameter Optimization\n",
        "\n",
        "Any XGBoost hyperparameter can be tuned, where the structure of the parameter dictionary needs to be as follows:\n",
        "\n",
        "    - Float/Int sample_type\n",
        "        - {\"param_name\": [\"sample_type\", low, high, log]}\n",
        "            - sample_type: str, Type of sampling, e.g., \"float\" or \"int\"\n",
        "            - low: int, Lower endpoint of the range of suggested values\n",
        "            - high: int, Upper endpoint of the range of suggested values\n",
        "            - log: bool, Flag to sample the value from the log domain or not\n",
        "        - Example: {\"eta\": \"float\", low=1e-5, high=1, log=True]}\n",
        "\n",
        "    - Categorical sample_type\n",
        "        - {\"param_name\": [\"sample_type\", [\"choice1\", \"choice2\", \"choice3\", \"...\"]]}\n",
        "            - sample_type: str, Type of sampling, either \"categorical\"\n",
        "            - choice1, choice2, choice3, ...: str, Possible choices for the parameter\n",
        "        - Example: {\"booster\": [\"categorical\", [\"gbtree\", \"dart\"]]}\n",
        "\n",
        "    - For parameters without tunable choice (this is needed if tree_method = \"gpu_hist\" and gpu_id needs to be specified)\n",
        "        - {\"param_name\": [\"none\", [value]]},\n",
        "            - param_name: str, Name of the parameter\n",
        "            - value: int, Value of the parameter\n",
        "        - Example: {\"gpu_id\": [\"none\", [0]]}\n",
        "\n",
        "Depending on which parameters are optimized, it might happen that some of them are not used, e.g., when {\"booster\":  [\"categorical\", [\"gbtree\", \"gblinear\"]]} and {\"max_depth\": [\"int\", 1, 10, False]} are specified, max_depth is not used when gblinear is sampled, since it has no such argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:05.890475500Z",
          "start_time": "2023-05-18T06:12:04.439051100Z"
        },
        "id": "1KyQrtZPtdXz"
      },
      "outputs": [],
      "source": [
        "param_dict = {\n",
        "    \"eta\":              [\"float\", {\"low\": 1e-5,   \"high\": 1,     \"log\": True}],\n",
        "    \"max_depth\":        [\"int\",   {\"low\": 1,      \"high\": 10,    \"log\": False}],\n",
        "    \"gamma\":            [\"float\", {\"low\": 1e-8,   \"high\": 40,    \"log\": True}],\n",
        "    \"subsample\":        [\"float\", {\"low\": 0.2,    \"high\": 1.0,   \"log\": False}],\n",
        "    \"colsample_bytree\": [\"float\", {\"low\": 0.2,    \"high\": 1.0,   \"log\": False}],\n",
        "    \"min_child_weight\": [\"float\", {\"low\": 1e-8,   \"high\": 500,   \"log\": True}],\n",
        "    \"booster\":          [\"categorical\", [\"gbtree\"]]\n",
        "}\n",
        "\n",
        "np.random.seed(123)\n",
        "opt_param = xgblss.hyper_opt(param_dict,\n",
        "                             dtrain,\n",
        "                             num_boost_round=100,        # Number of boosting iterations.\n",
        "                             nfold=5,                    # Number of cv-folds.\n",
        "                             early_stopping_rounds=20,   # Number of early-stopping rounds\n",
        "                             max_minutes=10,             # Time budget in minutes, i.e., stop study after the given number of minutes.\n",
        "                             n_trials=30 ,               # The number of trials. If this argument is set to None, there is no limitation on the number of trials.\n",
        "                             silence=True,               # Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.\n",
        "                             seed=123,                   # Seed used to generate cv-folds.\n",
        "                             hp_seed=123                 # Seed for random number generator used in the Bayesian hyperparameter search.\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwMPIMktdXz"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "We use the optimized hyper-parameters and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:06.612277900Z",
          "start_time": "2023-05-18T06:22:05.891472400Z"
        },
        "id": "VGNZHLUJtdXz"
      },
      "outputs": [],
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "opt_params = opt_param.copy()\n",
        "n_rounds = opt_params[\"opt_rounds\"]\n",
        "del opt_params[\"opt_rounds\"]\n",
        "\n",
        "# Train Model with optimized hyperparameters\n",
        "xgblss.train(opt_params,\n",
        "             dtrain,\n",
        "             num_boost_round=n_rounds\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNnVTCs3tdX0"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Similar to a XGBoost model, we now predict from the trained model. Different options are available:\n",
        "\n",
        "- `samples`: draws `n_samples` from the predicted distribution.\n",
        "- `quantiles`: calculates quantiles from the predicted distribution.\n",
        "- `parameters`: returns predicted distributional parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:06.942614600Z",
          "start_time": "2023-05-18T06:22:06.612277900Z"
        },
        "id": "HIQv9wCztdX0"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Number of samples to draw from predicted distribution\n",
        "n_samples = 1000\n",
        "quant_sel = [0.05, 0.95] # Quantiles to calculate from predicted distribution\n",
        "\n",
        "# Sample from predicted distribution\n",
        "pred_samples = xgblss.predict(dtest,\n",
        "                              pred_type=\"samples\",\n",
        "                              n_samples=n_samples,\n",
        "                              seed=123)\n",
        "\n",
        "# Calculate quantiles from predicted distribution\n",
        "pred_quantiles = xgblss.predict(dtest,\n",
        "                                pred_type=\"quantiles\",\n",
        "                                n_samples=n_samples,\n",
        "                                quantiles=quant_sel)\n",
        "\n",
        "# Return predicted distributional parameters\n",
        "pred_params = xgblss.predict(dtest,\n",
        "                             pred_type=\"parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:06.989477700Z",
          "start_time": "2023-05-18T06:22:06.942614600Z"
        },
        "id": "o5dhiAlFtdX0"
      },
      "outputs": [],
      "source": [
        "pred_samples.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:23:27.863941100Z",
          "start_time": "2023-05-18T06:23:27.837972100Z"
        },
        "id": "puwWEmJRtdX0"
      },
      "outputs": [],
      "source": [
        "pred_quantiles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:23:30.015801500Z",
          "start_time": "2023-05-18T06:23:29.988874300Z"
        },
        "id": "HrY8BxeStdX0"
      },
      "outputs": [],
      "source": [
        "pred_params.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNlCCZNvtdX0"
      },
      "source": [
        "# SHAP Interpretability\n",
        "\n",
        "To get a deeper understanding of the data generating process, XGBoostLSS also provides attribute importance and partial dependence plots using the Shapley-Value approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:07.616856700Z",
          "start_time": "2023-05-18T06:22:07.020722700Z"
        },
        "id": "P9s3JPwTtdX0"
      },
      "outputs": [],
      "source": [
        "# Partial Dependence Plot of how x_true acts on variance\n",
        "xgblss.plot(X_test,\n",
        "            parameter=\"scale\",\n",
        "            feature=\"x_true\",\n",
        "            plot_type=\"Partial_Dependence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:07.960311200Z",
          "start_time": "2023-05-18T06:22:07.616856700Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1fWO1woKtdX0"
      },
      "outputs": [],
      "source": [
        "# Feature Importance of scale parameter\n",
        "xgblss.plot(X_test,\n",
        "            parameter=\"scale\",\n",
        "            plot_type=\"Feature_Importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMFVSyLStdX1"
      },
      "source": [
        "# Plot of Actual vs. Predicted Quantiles\n",
        "\n",
        "In the following, we plot the predicted quantiles (blue) and compare them to the actual quantiles (dashed-black)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:08.982134200Z",
          "start_time": "2023-05-18T06:22:07.960311200Z"
        },
        "id": "3mQfV1fMtdX1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "###\n",
        "# Actual Quantiles\n",
        "###\n",
        "q1 = norm.ppf(quant_sel[0], loc = 10, scale = 1 + 4*((0.3 < test[\"x_true\"].values) & (test[\"x_true\"].values < 0.5)) + 2*(test[\"x_true\"].values > 0.7))\n",
        "q2 = norm.ppf(quant_sel[1], loc = 10, scale = 1 + 4*((0.3 < test[\"x_true\"].values) & (test[\"x_true\"].values < 0.5)) + 2*(test[\"x_true\"].values > 0.7))\n",
        "test[\"quant\"] = np.where(test[\"y\"].values < q1, 0, np.where(test[\"y\"].values < q2, 1, 2))\n",
        "test[\"alpha\"] = np.where(test[\"y\"].values <= q1, 1, np.where(test[\"y\"].values >= q2, 1, 0))\n",
        "df_quantiles = test[test[\"alpha\"] == 1]\n",
        "\n",
        "# Lower Bound\n",
        "yl = list(set(q1))\n",
        "yl.sort()\n",
        "yl = [yl[2],yl[0],yl[2],yl[1],yl[1]]\n",
        "sfunl = pd.DataFrame({\"x_true\":[0, 0.3, 0.5, 0.7, 1], \"y\":yl})\n",
        "\n",
        "# Upper Bound\n",
        "yu = list(set(q2))\n",
        "yu.sort()\n",
        "yu = [yu[0],yu[2],yu[0],yu[1],yu[1]]\n",
        "sfunu = pd.DataFrame({\"x_true\":[0, 0.3, 0.5, 0.7, 1], \"y\":yu})\n",
        "\n",
        "###\n",
        "# Predicted Quantiles\n",
        "###\n",
        "test[\"lb\"] = pred_quantiles.iloc[:,0]\n",
        "test[\"ub\"] = pred_quantiles.iloc[:,1]\n",
        "\n",
        "###\n",
        "# Plot\n",
        "###\n",
        "(ggplot(test,\n",
        "        aes(\"x_true\",\n",
        "            \"y\")) +\n",
        " geom_point(alpha = 0.2, color = \"black\", size = 2) +\n",
        " theme_bw(base_size=15) +\n",
        " theme(legend_position=\"none\",\n",
        "       plot_title = element_text(hjust = 0.5),\n",
        "       plot_subtitle = element_text(hjust = 0.5)) +\n",
        " labs(title = \"XGBoostLSS Regression - Simulated Data Example\",\n",
        "      subtitle = \"Comparison of Actual (black) vs. Predicted Quantiles (blue)\",\n",
        "      x=\"x\")  +\n",
        " geom_line(aes(\"x_true\",\n",
        "               \"ub\"),\n",
        "           size = 1,\n",
        "           color = \"blue\",\n",
        "           alpha = 0.7) +\n",
        " geom_line(aes(\"x_true\",\n",
        "               \"lb\"),\n",
        "           size = 1,\n",
        "           color = \"blue\",\n",
        "           alpha = 0.7) +\n",
        " geom_point(df_quantiles,\n",
        "            aes(\"x_true\",\n",
        "                \"y\"),\n",
        "            color = \"red\",\n",
        "            alpha = 0.7,\n",
        "            size = 2) +\n",
        " geom_step(sfunl,\n",
        "           aes(\"x_true\",\n",
        "               \"y\"),\n",
        "           size = 1,\n",
        "           linetype = \"dashed\")  +\n",
        " geom_step(sfunu,\n",
        "           aes(\"x_true\",\n",
        "               \"y\"),\n",
        "           size = 1,\n",
        "           linetype = \"dashed\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7YjcxdwtdX1"
      },
      "source": [
        "# True vs. Predicted Distributional Parameters\n",
        "\n",
        "In the following figure, we compare the true parameters of the Gaussian with the ones predicted by XGBoostLSS. The below figure shows that the estimated parameters closely match the true ones (recall that the location parameter $\\mu=10$ is simulated as being a constant)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-18T06:22:09.748483300Z",
          "start_time": "2023-05-18T06:22:08.982134200Z"
        },
        "id": "6fvF81fJtdX1"
      },
      "outputs": [],
      "source": [
        "pred_params[\"x_true\"] = X_test[\"x_true\"].values\n",
        "dist_params = list(xgblss.dist.param_dict.keys())\n",
        "\n",
        "# Data with actual values\n",
        "plot_df_actual = pd.melt(test[[\"x_true\"] + dist_params],\n",
        "                         id_vars=\"x_true\",\n",
        "                         value_vars=dist_params)\n",
        "plot_df_actual[\"type\"] = \"TRUE\"\n",
        "\n",
        "# Data with predicted values\n",
        "plot_df_predt = pd.melt(pred_params[[\"x_true\"] + dist_params],\n",
        "                        id_vars=\"x_true\",\n",
        "                        value_vars=dist_params)\n",
        "plot_df_predt[\"type\"] = \"PREDICT\"\n",
        "\n",
        "plot_df = pd.concat([plot_df_predt, plot_df_actual])\n",
        "\n",
        "plot_df[\"variable\"] = plot_df.variable.str.upper()\n",
        "plot_df[\"type\"] = pd.Categorical(plot_df[\"type\"], categories = [\"PREDICT\", \"TRUE\"])\n",
        "\n",
        "(ggplot(plot_df,\n",
        "        aes(x=\"x_true\",\n",
        "            y=\"value\",\n",
        "            color=\"type\")) +\n",
        " geom_line(size=1.1) +\n",
        " facet_wrap(\"variable\",\n",
        "            scales=\"free\") +\n",
        " labs(title=\"Parameters of univariate Gaussian predicted with XGBoostLSS\",\n",
        "      x=\"\",\n",
        "      y=\"\") +\n",
        " theme_bw(base_size=15) +\n",
        " theme(legend_position=\"bottom\",\n",
        "       plot_title = element_text(hjust = 0.5),\n",
        "       legend_title = element_blank())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWRUjwW9tdX1"
      },
      "source": [
        "# Actual vs. Predicted\n",
        "\n",
        "Since we predict the entire conditional distribution, we can overlay the point predictions with predicted densities, from which we can also derive quantiles of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGlaHZNttdX1"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "\n",
        "n_examples = 9\n",
        "\n",
        "for i in range(n_examples):\n",
        "    y_samples = pd.DataFrame(pred_samples.values[i,:].reshape(-1,1), columns=[\"PREDICT_DENSITY\"])\n",
        "    y_samples[\"PREDICT_POINT\"] = y_samples[\"PREDICT_DENSITY\"].mean()\n",
        "    y_samples[\"PREDICT_Q05\"] = y_samples[\"PREDICT_DENSITY\"].quantile(q=quant_sel[0])\n",
        "    y_samples[\"PREDICT_Q95\"] = y_samples[\"PREDICT_DENSITY\"].quantile(q=quant_sel[1])\n",
        "    y_samples[\"ACTUAL\"] = y_test[i]\n",
        "    y_samples[\"obs\"]= f\"Obervation {i+1}\"\n",
        "    y_pred.append(y_samples)\n",
        "\n",
        "pred_df = pd.melt(pd.concat(y_pred, axis=0), id_vars=\"obs\")\n",
        "pred_df[\"obs\"] = pd.Categorical(pred_df[\"obs\"], categories=[f\"Obervation {i+1}\" for i in range(n_examples)])\n",
        "df_actual, df_pred_dens, df_pred_point, df_q05, df_q95 = [x for _, x in pred_df.groupby(\"variable\")]\n",
        "\n",
        "plot_pred = (\n",
        "    ggplot(pred_df,\n",
        "           aes(color=\"variable\")) +\n",
        "    stat_density(df_pred_dens,\n",
        "                 aes(x=\"value\"),\n",
        "                 size=1.1) +\n",
        "    geom_point(df_pred_point,\n",
        "               aes(x=\"value\",\n",
        "                   y=0),\n",
        "               size=1.4) +\n",
        "    geom_point(df_actual,\n",
        "               aes(x=\"value\",\n",
        "                   y=0),\n",
        "               size=1.4) +\n",
        "    geom_vline(df_q05,\n",
        "               aes(xintercept=\"value\",\n",
        "                   fill=\"variable\",\n",
        "                   color=\"variable\"),\n",
        "               linetype=\"dashed\",\n",
        "               size=1.1) +\n",
        "    geom_vline(df_q95,\n",
        "               aes(xintercept=\"value\",\n",
        "                   fill=\"variable\",\n",
        "                   color=\"variable\"),\n",
        "               linetype=\"dashed\",\n",
        "               size=1.1) +\n",
        "    facet_wrap(\"obs\",\n",
        "               scales=\"free\",\n",
        "               ncol=3) +\n",
        "    labs(title=\"Predicted vs. Actual \\n\",\n",
        "         x = \"\") +\n",
        "    theme_bw(base_size=15) +\n",
        "    scale_fill_brewer(type=\"qual\", palette=\"Dark2\") +\n",
        "    theme(legend_position=\"bottom\",\n",
        "          plot_title = element_text(hjust = 0.5),\n",
        "          legend_title = element_blank()\n",
        "         )\n",
        ")\n",
        "\n",
        "print(plot_pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}